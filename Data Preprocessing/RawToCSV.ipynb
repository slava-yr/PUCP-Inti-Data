{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e94a16d",
   "metadata": {},
   "source": [
    "# Raw to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ece0e",
   "metadata": {},
   "source": [
    "Process the raw CSV files from the I-V Curve Tracer to get 1 CSV file with only the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987d1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85d54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(r\"D:\\Research\\Data\") # Base path for raw data files\n",
    "output_csv = Path(\"unfiltered_dataset.csv\") # Output CSV file path\n",
    "log_file = Path(\"processed_files.txt\") # Log file to keep track of processed files\n",
    "error_log = Path(\"error_log.txt\") # Log file for errors\n",
    "missing_csvs = Path(\"missing_csv.txt\") # Log missing csv filenames\n",
    "print_every = 2000 # Print progress every 2000 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232335a5",
   "metadata": {},
   "source": [
    "### Identified columns on CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b362b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_row1_column_names = [\n",
    "    'date', 'time_start', 'voltage_0', 'current_0', 'power_0', 'wavelength_0', 'spectralirr_0',\n",
    "    'modtemp_c', 'modtemp_l', 'cell_v', 'irr_horiz_start', 'irr_incl20_start', 'airtemp',\n",
    "    'humidity_rel', 'pressure_rel', 'air_density', 'wind_speed_kmh',\n",
    "    'wind_dir', 'humidity_abs', 'pressure_abs', 'wind_speed_ms', 'irr_east_start',\n",
    "    'irr_west_start', 'irr_floor_ref_start'\n",
    "]\n",
    "\n",
    "full_row1_column_names = base_row1_column_names + ['irr_diffuse_start', 'irr_incl15_start']\n",
    "\n",
    "base_row2_column_names = [\n",
    "    'module_name', 'time_end', 'voltage_1', 'current_1',\n",
    "    'power_1', 'wavelength_1', 'spectralirr_1', 'cell_v_end',\n",
    "    'irr_horiz_end', 'irr_incl20_end', 'irr_east_end', 'irr_west_end',\n",
    "    'irr_floor_ref_end']\n",
    "\n",
    "full_row2_column_names = base_row2_column_names + ['irr_diffuse_end', 'irr_incl15_end']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566222a3",
   "metadata": {},
   "source": [
    "### Error log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999433c",
   "metadata": {},
   "source": [
    "This will log errors into a file with the corresponding exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2ddbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(filename, message, exception=None):\n",
    "    if exception:\n",
    "        full_msg = f\"{filename} — {message}: {repr(exception)}\"\n",
    "    else:\n",
    "        full_msg = f\"{filename} — {message}\"\n",
    "\n",
    "    # Print to console\n",
    "    print(\"❌\", full_msg)\n",
    "\n",
    "    # Append to log file\n",
    "    with open(error_log, \"a\", encoding=\"utf-8\") as err_f:\n",
    "        err_f.write(full_msg + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5ad14",
   "metadata": {},
   "source": [
    "# Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d294772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    filename = file_path.name\n",
    "\n",
    "    # --- Robust Read Logic from V3 Script ---\n",
    "    opts = dict(sep=';', header=None, na_values='--', engine='python')\n",
    "    try:\n",
    "        # Try default UTF-8 first\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', encoding_errors='strict', **opts)\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to latin1 if UTF-8 fails (handles the \\xe3 byte)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='latin1', encoding_errors='replace', **opts)\n",
    "        except Exception as e:\n",
    "            log_error(filename, \"Error reading CSV even with fallback\", e)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Error reading CSV\", e)\n",
    "        return None\n",
    "\n",
    "    # Check number of columns\n",
    "    num_cols = df.shape[1]\n",
    "    if num_cols == 26:\n",
    "        current_row1_names = full_row1_column_names\n",
    "        current_row2_names = full_row2_column_names\n",
    "    elif num_cols == 24:\n",
    "        current_row1_names = base_row1_column_names\n",
    "        current_row2_names = base_row2_column_names\n",
    "    else:\n",
    "        log_error(filename, f\"Unexpected column count: {num_cols}\", None)\n",
    "        return None\n",
    "    \n",
    "    # Parse Row 1\n",
    "    try:\n",
    "        row1_list = df.iloc[0, :].dropna().tolist()\n",
    "        # Check if dropna produced the correct length\n",
    "        if len(row1_list) == len(current_row1_names):\n",
    "            df_row1 = pd.DataFrame([row1_list], columns=current_row1_names)\n",
    "        else:\n",
    "            # Fallback for Row 1\n",
    "            df_row1 = pd.DataFrame([df.iloc[0, :len(current_row1_names)].tolist()], columns=current_row1_names)\n",
    "        row1 = df_row1.iloc[0]\n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Error parsing Row 1 metadata\", e)\n",
    "        return None\n",
    "\n",
    "    # Parse Row 2\n",
    "    try:\n",
    "        row2_list = df.iloc[1, :].dropna().tolist()\n",
    "        # Check if dropna produced the correct length (13 or 15)\n",
    "        if len(row2_list) == len(current_row2_names):\n",
    "            df_row2 = pd.DataFrame([row2_list], columns=current_row2_names)\n",
    "        else:\n",
    "            # Fallback for Row 2: Explicitly pull indices to handle the gaps\n",
    "            row2_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 21, 22, 23]\n",
    "            if num_cols == 26:\n",
    "                row2_idxs += [24, 25] # Add Diffuse and GTI 15 for 26-col files\n",
    "            \n",
    "            df_row2 = pd.DataFrame([df.iloc[1, row2_idxs].tolist()], columns=current_row2_names)\n",
    "        row2 = df_row2.iloc[0]\n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Error parsing Row 2 metadata\", e)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Generate timestamp from filename\n",
    "        name = filename.replace('.csv', '')\n",
    "        # Split by underscore\n",
    "        parts = name.split('_')\n",
    "        # Extract date/time parts (skip module name)\n",
    "        day = int(parts[1])\n",
    "        month = int(parts[2])\n",
    "        year = int(parts[3])\n",
    "        hour = int(parts[4])\n",
    "        minute = int(parts[5])\n",
    "        second_str = parts[6].split(' ')[0]\n",
    "        second = int(second_str)\n",
    "        \n",
    "        timestamp = datetime(year, month, day, hour, minute, second)\n",
    "        \n",
    "        module_name = row2[\"module_name\"]\n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Missing metadata fields\", e)\n",
    "        return None\n",
    "    \n",
    "    # Compute Spectral Integral\n",
    "    try:\n",
    "        wavelength = df.iloc[:2048, 5].copy() # Columns 5 is wavelength (nm) \n",
    "        spectral_irradiance = df.iloc[:2048, 6].copy() # Column 6 is spectral irradiance (W/m2/nm)\n",
    "        computed_irradiance = np.trapezoid(spectral_irradiance, x=wavelength)\n",
    "    except Exception as e:\n",
    "        computed_irradiance = np.nan\n",
    "        # log_error(filename, \"Error computing spectral integral\", e)\n",
    "        # return None\n",
    "\n",
    "    # Get Irradiances\n",
    "    try:\n",
    "        # Start irradiances (from row1)\n",
    "        irr_horiz_start = row1[\"irr_horiz_start\"]\n",
    "        irr_incl20_start = row1[\"irr_incl20_start\"]\n",
    "        irr_incl15_start = row1.get(\"irr_incl15_start\", np.nan)\n",
    "        irr_east_start = row1[\"irr_east_start\"]\n",
    "        irr_west_start = row1[\"irr_west_start\"]\n",
    "        irr_floor_ref_start = row1[\"irr_floor_ref_start\"]\n",
    "        \n",
    "        # End irradiances (from row2)\n",
    "        irr_horiz_end = row2[\"irr_horiz_end\"]\n",
    "        irr_incl20_end = row2[\"irr_incl20_end\"]\n",
    "        irr_incl15_end = row2.get(\"irr_incl15_end\", np.nan)\n",
    "        irr_east_end = row2[\"irr_east_end\"]\n",
    "        irr_west_end = row2[\"irr_west_end\"]\n",
    "        irr_floor_ref_end = row2[\"irr_floor_ref_end\"]\n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Missing irradiance data\", e)\n",
    "        return None\n",
    "\n",
    "    # Get temperature\n",
    "    try:\n",
    "        modtemp_c = row1[\"modtemp_c\"]\n",
    "        modtemp_l = row1[\"modtemp_l\"]\n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Error reading module temperatures\", e)\n",
    "        return None\n",
    "\n",
    "    # I-V & P-V Curve validation\n",
    "    try: \n",
    "        measurements = df.iloc[:, [2, 3, 4]].copy()\n",
    "        measurements.columns = ['voltage', 'current', 'power']\n",
    "        # Discard the first values (start from voltage close to 0)\n",
    "        min_idx = measurements['voltage'].idxmin()\n",
    "        measurements = measurements.iloc[min_idx:].reset_index(drop=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Error processing I-V curve\", e)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        V = measurements[\"voltage\"]\n",
    "        I = measurements[\"current\"]\n",
    "        P = measurements[\"power\"]\n",
    "\n",
    "        Voc = V.max()\n",
    "        Pmpp = P.max()\n",
    "        mpp_idx = P.idxmax()\n",
    "        Vmpp = V[mpp_idx]\n",
    "        Impp = I[mpp_idx]\n",
    "        Imin = I.min()\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Error computing electrical parameters\", e)\n",
    "        return None\n",
    "\n",
    "    # Delete tail\n",
    "    try:\n",
    "        d = np.mean(np.abs(np.diff(I[:200])))\n",
    "        diffs_20 = np.abs(np.diff(I[:20]))\n",
    "        # Compare diffs20 to d\n",
    "        idxs_cola = [i for i, diff in enumerate(diffs_20) if diff > d]\n",
    "        if idxs_cola:\n",
    "            corte_inicial = idxs_cola[-1] + 1\n",
    "            measurements = measurements.iloc[corte_inicial:].reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Error cleaning up cola noise\", e)\n",
    "\n",
    "    # Find Isc using linear regression on the initial segment (V < Voc/4)\n",
    "    try:\n",
    "        # Define the threshold for the initial \"flat\" segment of the curve\n",
    "        # Using the Voc previously computed in your function\n",
    "        v_limit = Voc / 4 \n",
    "        \n",
    "        # Select points below the threshold\n",
    "        mask_ini = (measurements[\"voltage\"] < v_limit)\n",
    "        \n",
    "        # Ensure we have enough points (at least 3) to perform a stable regression\n",
    "        if mask_ini.sum() >= 3:\n",
    "            vx = measurements.loc[mask_ini, \"voltage\"].values\n",
    "            iy = measurements.loc[mask_ini, \"current\"].values\n",
    "            \n",
    "            # Perform Linear Regression: I = a*V + b\n",
    "            # A becomes a matrix of [Voltage, 1]\n",
    "            A = np.vstack([vx, np.ones_like(vx)]).T\n",
    "            a, b = np.linalg.lstsq(A, iy, rcond=None)[0]\n",
    "            \n",
    "            # The intercept 'b' is the estimated current at V=0 (Isc)\n",
    "            Isc = float(b)\n",
    "        else:\n",
    "            # Fallback for very sparse curves: take the current closest to V=0\n",
    "            ordered_meas = measurements.sort_values(by=\"voltage\")\n",
    "            Isc = float(ordered_meas[\"current\"].iloc[0])\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Error estimating Isc via linear regression\", e)\n",
    "        return None\n",
    "    \n",
    "    # Build final dataframe\n",
    "    try:\n",
    "        final_df = pd.DataFrame({\n",
    "            # Metadata\n",
    "            'filename': [filename],\n",
    "            'module_name': [module_name],\n",
    "            'timestamp': [timestamp],\n",
    "\n",
    "            # MPP\n",
    "            'Vmpp': [Vmpp],\n",
    "            'Impp': [Impp],\n",
    "            'Pmpp': [Pmpp],\n",
    "\n",
    "            # Electrical characteristics\n",
    "            'Voc': [Voc],\n",
    "            'Imin': [Imin],\n",
    "            'Isc': [Isc],\n",
    "\n",
    "            # Irradiance\n",
    "            'G_spec_int': [computed_irradiance],\n",
    "            'G_tilt20_start': [irr_incl20_start],\n",
    "            'G_tilt15_start': [irr_incl15_start],\n",
    "            'G_horiz_start': [irr_horiz_start],\n",
    "            'G_east_start': [irr_east_start],\n",
    "            'G_west_start': [irr_west_start],\n",
    "            'G_refl_start': [irr_floor_ref_start],\n",
    "\n",
    "            'G_horiz_end': [irr_horiz_end],\n",
    "            'G_tilt20_end': [irr_incl20_end],\n",
    "            'G_tilt15_end': [irr_incl15_end],\n",
    "            'G_east_end': [irr_east_end],\n",
    "            'G_west_end': [irr_west_end],\n",
    "            'G_refl_end': [irr_floor_ref_end],\n",
    "\n",
    "            # Environmental characteristics\n",
    "            'module_temperature_center': [modtemp_c],\n",
    "            'module_temperature_lateral': [modtemp_l],\n",
    "            'air_temperature': [row1['airtemp']],\n",
    "            'relative_humidity': [row1['humidity_rel']],\n",
    "            'air_density': [row1['air_density']],\n",
    "            'abs_pressure': [row1['pressure_abs']],\n",
    "            'wind_speed_ms': [row1['wind_speed_ms']],\n",
    "            'wind_direction': [row1['wind_dir']]\n",
    "        })\n",
    "    except Exception as e:\n",
    "        log_error(filename, \"Error creating final dataframe\", e)\n",
    "        return None\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eaf5151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 419197 files in 2022\n",
      "Found 425151 files in 2023\n",
      "Found 440615 files in 2024\n",
      "Found 424753 files in 2025\n",
      "\n",
      "Total: 1709716 CSV files from 2022-2025 (excluding MS711, WS500, PV)\n",
      "Already processed: 1707840 files\n",
      "Files to process: 732\n"
     ]
    }
   ],
   "source": [
    "# Get all CSV files from 2023-2025\n",
    "all_files = []\n",
    "skip_prefixes = ('MS711', 'WS500', 'PV')\n",
    "\n",
    "for year in range(2022, 2026):  # 2022-2025\n",
    "    year_files = list(base_path.glob(f\"{year}/**/*.csv\"))\n",
    "    # Filter out unwanted prefixes\n",
    "    year_files = [f for f in year_files if not f.name.startswith(skip_prefixes)]\n",
    "    all_files.extend(year_files)\n",
    "    print(f\"Found {len(year_files)} files in {year}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_files)} CSV files from 2022-2025 (excluding MS711, WS500, PV)\")\n",
    "\n",
    "# Load already processed files\n",
    "processed_files = set()\n",
    "if log_file.exists():\n",
    "    with open(log_file, 'r') as f:\n",
    "        processed_files = set(line.strip() for line in f)\n",
    "    print(f\"Already processed: {len(processed_files)} files\")\n",
    "\n",
    "# Filter to only new files\n",
    "files_to_process = [f for f in all_files if f.name not in processed_files]\n",
    "total_files = len(files_to_process)\n",
    "\n",
    "print(f\"Files to process: {total_files}\")\n",
    "\n",
    "# Determine if we need header\n",
    "write_header = not output_csv.exists() or output_csv.stat().st_size == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99848127",
   "metadata": {},
   "source": [
    "# Process Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c18b12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ CDF1150A1_26_04_2022_13_11_29.csv — Unexpected column count: 18\n",
      "❌ CDF1150A1_26_04_2022_13_16_29.csv — Unexpected column count: 18\n",
      "❌ DUSTCS6K270P_26_04_2022_13_10_01.csv — Unexpected column count: 18\n",
      "❌ DUSTCS6K270P_26_04_2022_13_15_01.csv — Unexpected column count: 18\n",
      "❌ GSA060_26_04_2022_13_11_07.csv — Unexpected column count: 18\n",
      "❌ GSA060_26_04_2022_13_16_07.csv — Unexpected column count: 18\n",
      "❌ LG345N1C_26_04_2022_13_10_45.csv — Unexpected column count: 18\n",
      "❌ LG345N1C_26_04_2022_13_15_45.csv — Unexpected column count: 18\n",
      "❌ LG370Q1C_26_04_2022_13_10_23.csv — Unexpected column count: 18\n",
      "❌ LG370Q1C_26_04_2022_13_15_23.csv — Unexpected column count: 18\n",
      "❌ _26_04_2022_12_21_51.csv — Unexpected column count: 17\n",
      "❌ _26_04_2022_12_22_13.csv — Unexpected column count: 17\n",
      "❌ _26_04_2022_13_11_51.csv — Unexpected column count: 17\n",
      "❌ _26_04_2022_13_12_13.csv — Unexpected column count: 17\n",
      "❌ _26_04_2022_13_13_05.csv — Unexpected column count: 17\n",
      "❌ _10_06_2022___.csv — Unexpected column count: 7\n",
      "[1/732] processed (0.1%) — Skipped: 16 — ETA: 5.3 min\n",
      "❌ CS3K320MB-T_10_01_2024_06_44_03.csv — Error reading CSV even with fallback: ParserError('Expected 26 fields in line 3, saw 27')\n",
      "❌ _27_01_2024_12_40_01.csv — Unexpected column count: 21\n",
      "❌ _27_01_2024___.csv — Unexpected column count: 12\n",
      "❌ _31_01_2024_15_14_25.csv — Unexpected column count: 21\n",
      "❌ _02_02_2024_15_26_16.csv — Unexpected column count: 21\n",
      "❌ _09_02_2024_10_00_01.csv — Unexpected column count: 21\n",
      "❌ _09_02_2024_14_41_16.csv — Unexpected column count: 21\n",
      "❌ _09_02_2024_18_00_19.csv — Unexpected column count: 21\n",
      "❌ _19_02_2024_07_29_24.csv — Unexpected column count: 21\n",
      "❌ _29_02_2024_13_42_11.csv — Unexpected column count: 21\n",
      "❌ _29_02_2024_13_49_03.csv — Unexpected column count: 21\n",
      "❌ _27_03_2024_12_49_03.csv — Unexpected column count: 21\n",
      "❌ QPEAK315_02_07_2024_08_56_16.csv — Error reading CSV: ParserError('field larger than field limit (131072)')\n",
      "❌ _15_10_2024_17_00_01.csv — Unexpected column count: 21\n",
      "❌ _15_10_2024_17_00_19.csv — Unexpected column count: 21\n",
      "❌ _16_10_2024_16_06_16.csv — Unexpected column count: 21\n",
      "❌ _16_10_2024_17_27_11.csv — Unexpected column count: 21\n",
      "❌ _16_10_2024___.csv — Unexpected column count: 12\n",
      "❌ _17_10_2024_07_34_06.csv — Unexpected column count: 21\n",
      "❌ _17_10_2024_07_34_26.csv — Unexpected column count: 21\n",
      "❌ _24_10_2024_13_41_16.csv — Unexpected column count: 21\n",
      "❌ _24_10_2024_15_33_04.csv — Unexpected column count: 21\n",
      "❌ _26_11_2024_15_28_04.csv — Unexpected column count: 21\n",
      "❌ _26_11_2024_15_31_53.csv — Unexpected column count: 21\n",
      "❌ _26_11_2024_15_45_01.csv — Unexpected column count: 21\n",
      "❌ _26_11_2024___.csv — Unexpected column count: 12\n",
      "❌ _28_11_2024_06_13_04.csv — Unexpected column count: 21\n",
      "❌ _28_03_2025_19_18_00.csv — Unexpected column count: 21\n",
      "❌ _28_03_2025_19_20_00.csv — Unexpected column count: 21\n",
      "❌ _28_03_2025_19_40_00.csv — Unexpected column count: 21\n",
      "❌ _06_05_2025_11_31_16.csv — Unexpected column count: 21\n",
      "❌ _06_05_2025_11_34_06.csv — Unexpected column count: 21\n",
      "❌ _06_05_2025_11_34_26.csv — Unexpected column count: 21\n",
      "❌ _06_05_2025___.csv — Unexpected column count: 12\n",
      "❌ _03_09_2025_17_32_19.csv — Unexpected column count: 21\n",
      "❌ CHSM78N630_15_10_2025_15_59_06.csv — Error reading CSV: ParserError('Expected 26 fields in line 4, saw 27')\n",
      "❌ _17_10_2025_17_33_04.csv — Unexpected column count: 21\n",
      "❌ _18_10_2025_16_16_16.csv — Unexpected column count: 21\n",
      "❌ LG370Q1C_19_10_2025_09_31_35.csv — Error processing I-V curve: TypeError(\"'<' not supported between instances of 'float' and 'str'\")\n",
      "❌ CHSM78N630_20_10_2025_14_39_06.csv — Error reading CSV: ParserError('Expected 26 fields in line 4, saw 27')\n",
      "❌ CS6K270P_20_10_2025_13_40_19.csv — Error reading CSV: ParserError('Expected 26 fields in line 4, saw 27')\n",
      "❌ LG345N1C_20_10_2025_12_21_53.csv — Error reading CSV even with fallback: ParserError('Expected 26 fields in line 4, saw 27')\n",
      "❌ LG345N1C_20_10_2025_13_26_53.csv — Error reading CSV: ParserError('Expected 26 fields in line 6, saw 27')\n",
      "❌ _22_10_2025_14_23_04.csv — Unexpected column count: 21\n",
      "❌ _22_10_2025_14_58_04.csv — Unexpected column count: 21\n",
      "❌ _22_10_2025___.csv — Unexpected column count: 12\n",
      "❌ CIEH5C1325-V_25_12_2025_11_55_37.csv — Error reading CSV: ParserError('Expected 26 fields in line 5, saw 27')\n",
      "❌ QPEAK315_28_12_2025_14_46_16.csv — Error reading CSV even with fallback: ParserError('Expected 26 fields in line 6, saw 27')\n",
      "\n",
      "✅ Extraction complete. Processed: 668, Skipped due to error: 64\n"
     ]
    }
   ],
   "source": [
    "processed_count = 0\n",
    "skipped_due_to_error = 0\n",
    "start_time = time.time()\n",
    "\n",
    "with open(output_csv, 'a', newline='', encoding='utf-8') as output_f:\n",
    "    for i, file_path in enumerate(files_to_process):  # ✅ Use files_to_process\n",
    "        filename = file_path.name  # ✅ Just filename for consistency\n",
    "        \n",
    "        try:\n",
    "            result_df = process_file(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Unhandled error on file: {filename}\")\n",
    "            skipped_due_to_error += 1\n",
    "            continue\n",
    "\n",
    "        if result_df is None:\n",
    "            skipped_due_to_error += 1\n",
    "            continue\n",
    "\n",
    "        result_df.to_csv(output_f, header=write_header, index=False)\n",
    "        write_header = False\n",
    "\n",
    "        with open(log_file, 'a') as log_f:\n",
    "            log_f.write(filename + '\\n')  \n",
    "\n",
    "        processed_count += 1\n",
    "\n",
    "        # Progress print\n",
    "        if processed_count % print_every == 0 or processed_count == 1:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / processed_count\n",
    "            remaining = avg_time * (total_files - processed_count)\n",
    "            print(\n",
    "                f\"[{processed_count}/{total_files}] processed \"\n",
    "                f\"({processed_count/total_files * 100:.1f}%) \"\n",
    "                f\"— Skipped: {skipped_due_to_error} \"\n",
    "                f\"— ETA: {remaining / 60:.1f} min\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n✅ Extraction complete. Processed: {processed_count}, Skipped due to error: {skipped_due_to_error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
